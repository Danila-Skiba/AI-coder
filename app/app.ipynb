{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e74e6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models.gigachat import GigaChat\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_gigachat.chat_models import GigaChat\n",
    "from langchain_gigachat.embeddings.gigachat import GigaChatEmbeddings\n",
    "from pathlib import Path\n",
    "from vectorization.v_a_c import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4238097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"YjllY2FhYjgtNGRlMC00MDA4LWIwZmYtNjdlNjY0ZmI5OTc4OmRkMjZhOWFjLThhNTctNGM3ZC1iZjFkLWQ3NGY1NmRjNTQzMQ==\"\n",
    "CODE_DIR = Path(\"..\\\\data\\\\code\")\n",
    "DOC_DIR = Path(\"..\\\\data\\\\documentation\")\n",
    "VECTOR_STORE_PATH = \"langchain_vector_store\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "987d8ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=GigaChatEmbeddings(\n",
    "        credentials=API_KEY,\n",
    "        scope=\"GIGACHAT_API_PERS\",\n",
    "        verify_ssl_certs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "072f5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GigaChatEmbeddings(\n",
    "    credentials=API_KEY,\n",
    "    verify_ssl_certs=False\n",
    ")\n",
    "\n",
    "system = SmartCodeDocSystem(embeddings, chunk_size=600, chunk_overlap=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a53e796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка и обработка файлов...\n",
      "Обработка кода из ..\\data\\code\n",
      "  Обработано 10/19 файлов кода\n",
      "Обработка документации из ..\\data\\documentation\n",
      "  Обработано 5/17 файлов документации\n",
      "  Обработано 10/17 файлов документации\n",
      "  Обработано 15/17 файлов документации\n",
      "Всего создано 406 чанков\n",
      "  - Код: 98\n",
      "  - Документация: 308\n",
      "Создание векторного хранилища...\n",
      "Сохранение в langchain_vector_store\n",
      "Векторное хранилище создано и сохранено\n"
     ]
    }
   ],
   "source": [
    "documents = system.load_and_process_files(CODE_DIR, DOC_DIR)\n",
    "\n",
    "system.create_vector_store(VECTOR_STORE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d04605c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка векторного хранилища из langchain_vector_store\n",
      "Векторное хранилище загружено\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system.load_vector_store(VECTOR_STORE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dff123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GigaChat(\n",
    "    credentials=API_KEY,\n",
    "    verify_ssl_certs=False,\n",
    ")\n",
    "\n",
    "smart_retriever = SmartRetriever(smart_system=system, k=3)\n",
    "\n",
    "prompt = create_smart_prompt()\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "retrieval_chain = create_retrieval_chain(smart_retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c8da407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ОТВЕТ:\n",
      "### Как использовать FAISS векторный магазин в LangChain?\n",
      "\n",
      "FAISS (Fast Approximate Nearest Neighbor Search) — это популярная библиотека для приближенного поиска ближайших соседей в пространстве векторов. В LangChain эта библиотека может быть использована через соответствующий интеграционный модуль. Вот как это можно сделать:\n",
      "\n",
      "#### Шаг 1: Установка необходимых пакетов\n",
      "Для начала установим необходимые зависимости:\n",
      "\n",
      "```bash\n",
      "% pip install --upgrade --quiet  langchain-openai faiss-cpu\n",
      "```\n",
      "\n",
      "Это установит LangChain с поддержкой OpenAI API, а также необходимую версию библиотеки FAISS для CPU.\n",
      "\n",
      "#### Шаг 2: Пример использования FAISS в LangChain\n",
      "\n",
      "Рассмотрим простой пример, где мы загружаем предварительно обученные векторы и используем их для поиска ближайшего соседа.\n",
      "\n",
      "```python\n",
      "from langchain import LLM, Chain, FaissIndexer\n",
      "import faiss\n",
      "\n",
      "# Загрузим предварительно обученные векторы\n",
      "vectors = faiss.read_index(\"path/to/your/embedding/file.index\")\n",
      "\n",
      "# Создадим индексатор на основе этих векторов\n",
      "indexer = FaissIndexer(vectors)\n",
      "\n",
      "# Теперь создадим цепочку для поиска ближайшего соседа\n",
      "llm = LLM(\"openai\", \"davinci\")\n",
      "chain = Chain(llm, indexer)\n",
      "\n",
      "# Найдем ближайший вектор для заданного вектора\n",
      "query_vector = ...  # Вектор запроса\n",
      "nearest_neighbor = chain.run(query_vector)\n",
      "\n",
      "print(f\"Nearest neighbor: {nearest_neighbor}\")\n",
      "```\n",
      "\n",
      "#### Объяснение шагов:\n",
      "\n",
      "1. **Загрузка векторов**: Мы читаем файл `.index`, который содержит векторы, обученные ранее. Этот файл обычно генерируется после обучения модели векторного представления слов.\n",
      "   \n",
      "2. **Создание индексатора**: Индексатор `FaissIndexer` принимает загруженные векторы и создает структуру данных для быстрого поиска ближайших соседей.\n",
      "\n",
      "3. **Использование цепочки**: Цепочка состоит из нескольких компонентов, включая языковую модель (LLM), которая используется для анализа запросов, и индексатор векторов.\n",
      "\n",
      "4. **Поиск ближайшего соседа**: Для этого передается запрос вектор, и результат возвращает ближайший вектор из индексированных.\n",
      "\n",
      "#### Примечания:\n",
      "- Файл `.index` должен быть совместим с использованной версией FAISS. Обычно этот формат поддерживается многими библиотеками, которые обучают векторы слов.\n",
      "- Для получения результатов быстрее всего будет использовать GPU-версию FAISS, если она доступна.\n",
      "\n",
      "### Итоги\n",
      "\n",
      "В примере показано, как интегрировать FAISS в LangChain для выполнения поиска ближайших соседей среди векторов слов. Это позволяет эффективно работать с большими наборами данных и значительно ускоряет процесс обработки естественного языка благодаря быстрым алгоритмам поиска.\n",
      "\n",
      "ИНФО О ПОИСКЕ:\n",
      "Тип поиска: doc-first\n",
      "Найдено документов: 2\n",
      "Код: 0, Документация: 2\n"
     ]
    }
   ],
   "source": [
    "query = \"How to use FAISS vectorstore in LangChain?\"\n",
    "\n",
    "try:\n",
    "    # Получаем ответ\n",
    "    response = retrieval_chain.invoke({\"input\": query})\n",
    "    \n",
    "    # Выводим результат\n",
    "    print(\"ОТВЕТ:\")\n",
    "    print(response[\"answer\"])\n",
    "    \n",
    "    # Показываем информацию о поиске\n",
    "    search_result = system.smart_search(query, k=3)\n",
    "    print(f\"\\nИНФО О ПОИСКЕ:\")\n",
    "    print(f\"Тип поиска: {search_result.search_type}\")\n",
    "    print(f\"Найдено документов: {len(search_result.documents)}\")\n",
    "    \n",
    "    code_count = len([d for d in search_result.documents if d.metadata.get('type') == 'code'])\n",
    "    doc_count = len([d for d in search_result.documents if d.metadata.get('type') == 'doc'])\n",
    "    print(f\"Код: {code_count}, Документация: {doc_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Ошибка: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f97dc21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ОТВЕТ:\n",
      "### Как работает поиск схожести в FAISS?\n",
      "\n",
      "FAISS (Fast Approximate Nearest neigbor Search) — это высокопроизводительная библиотека для индексации и поиска ближайших соседей в пространстве признаков. Она поддерживает различные методы индексирования и позволяет эффективно искать похожие элементы в больших наборах данных.\n",
      "\n",
      "#### 1. Принципы работы FAISS\n",
      "\n",
      "FAISS использует подход k-d деревьев (k-мерных деревьев) для построения иерархической структуры, которая помогает быстро находить ближайшие точки в многомерном пространстве. Основные шаги поиска включают:\n",
      "\n",
      "1. **Инициализация дерева**: Фактически, дерево строится как иерархическая структура, где каждый уровень разбивает пространство на два подпространства с помощью гиперплоскостей. Это позволяет быстро сужать область поиска.\n",
      "   \n",
      "2. **Вставка точек**: Каждая точка вставляется в соответствующее место в дереве. В зависимости от выбранного метода, может использоваться несколько вариантов разбиения пространства.\n",
      "\n",
      "3. **Поиск ближайшего соседа**: Для поиска наиболее близких точек алгоритм использует метрику расстояния, такую как L2 или Cosine Similarity, и продвигается вниз по дереву, проверяя только те ветви, которые могут содержать подходящие результаты.\n",
      "\n",
      "4. **Оптимизации**: FAISS поддерживает множество оптимизаций, таких как разделение пространства по векторам, использование пространственных вложенных структур (например, PQ, IVFPQ), что значительно ускоряет процесс поиска.\n",
      "\n",
      "#### 2. Пример использования FAISS\n",
      "\n",
      "Для того чтобы использовать FAISS в рамках библиотеки LangChain, необходимо создать объект, который будет управлять индексом. Например, предположим, что у нас есть модель, возвращающая векторные представления для каждого документа, и мы хотим найти наиболее похожие документы в базе данных. Вот как это можно сделать:\n",
      "\n",
      "```python\n",
      "from langchain import LLM, FaissIndexer\n",
      "\n",
      "# Инициализация модели\n",
      "llm = LLM(...)\n",
      "\n",
      "# Создание индексатора\n",
      "indexer = FaissIndexer(llm, index_key=\"my_index\")\n",
      "\n",
      "# Добавление документов в индекс\n",
      "documents = [\"Это первый документ\", \"А этот второй\"]\n",
      "indexer.add_documents(documents)\n",
      "\n",
      "# Поиск ближайших соседей\n",
      "query = \"Какой-то запрос\"\n",
      "similar_documents = indexer.find_nearest_neighbors(query)\n",
      "\n",
      "print(\"Наиболее похожие документы:\", similar_documents)\n",
      "```\n",
      "\n",
      "### Противоречия между документацией и кодом\n",
      "\n",
      "В данном случае, нет явных противоречий между документацией и кодом, так как код демонстрирует практическое применение FAISS через класс `FaissIndexer`, который управляет процессом создания индекса и поиска ближайших соседей. Документация предоставляет достаточно информации о том, как именно происходит работа с FAISS, включая возможные параметры настройки.\n",
      "\n",
      "ИНФО О ПОИСКЕ:\n",
      "Тип поиска: code-first\n",
      "Найдено документов: 1\n",
      "Код: 0, Документация: 1\n"
     ]
    }
   ],
   "source": [
    "query = \"How does similarity search work in FAISS?\"\n",
    "\n",
    "try:\n",
    "    # Получаем ответ\n",
    "    response = retrieval_chain.invoke({\"input\": query})\n",
    "    \n",
    "    # Выводим результат\n",
    "    print(\"ОТВЕТ:\")\n",
    "    print(response[\"answer\"])\n",
    "    \n",
    "    # Показываем информацию о поиске\n",
    "    search_result = system.smart_search(query, k=3)\n",
    "    print(f\"\\nИНФО О ПОИСКЕ:\")\n",
    "    print(f\"Тип поиска: {search_result.search_type}\")\n",
    "    print(f\"Найдено документов: {len(search_result.documents)}\")\n",
    "    \n",
    "    code_count = len([d for d in search_result.documents if d.metadata.get('type') == 'code'])\n",
    "    doc_count = len([d for d in search_result.documents if d.metadata.get('type') == 'doc'])\n",
    "    print(f\"Код: {code_count}, Документация: {doc_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Ошибка: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aa754b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ОТВЕТ:\n",
      "Документ в LangChain представляет собой универсальный способ хранения неструктурированного текста вместе с его метаданными. Он используется как базовый тип данных в различных методах обработки естественного языка, таких как классификация текста, создание моделей с нуля и т. д. \n",
      "\n",
      "### Как создать документ в LangChain?\n",
      "\n",
      "Документ в LangChain создается с помощью класса `Document`. Чтобы создать объект `Document`, необходимо передать два обязательных аргумента: текст (страницы контента) и словарь с метаданными. Вот как это выглядит на практике:\n",
      "\n",
      "```python\n",
      "from langchain import LLM, Document\n",
      "\n",
      "# Создание объекта Document с текстом и метаданными\n",
      "doc = Document(\n",
      "    page_content=\"Это пример содержимого страницы\",\n",
      "    metadata={\"author\": \"Иван Иванов\", \"topic\": \"Программирование\"}\n",
      ")\n",
      "```\n",
      "\n",
      "В данном примере:\n",
      "- `page_content` — это строка, содержащая текст страницы.\n",
      "- `metadata` — это словарь, который содержит дополнительные данные о документе, такие как автор, тема и любые другие поля, которые могут быть полезны для последующей обработки.\n",
      "\n",
      "### Пример создания нескольких документов\n",
      "\n",
      "Можно создавать несколько документов сразу, передавая их в виде списка объектов `Document`:\n",
      "\n",
      "```python\n",
      "documents = [\n",
      "    Document(\n",
      "        page_content=\"Первый документ\",\n",
      "        metadata={\"author\": \"Алексей Алексеев\", \"topic\": \"Финансы\"}\n",
      "    ),\n",
      "    Document(\n",
      "        page_content=\"Второй документ\",\n",
      "        metadata={\"author\": \"Мария Иванова\", \"topic\": \"Литература\"}\n",
      "    )\n",
      "]\n",
      "```\n",
      "\n",
      "### Использование методов `add_documents`\n",
      "\n",
      "Для добавления документов в модель или другую структуру можно использовать метод `add_documents`. Этот метод принимает список документов или отдельных документов и сохраняет их в соответствующем хранилище. Например:\n",
      "\n",
      "```python\n",
      "from langchain import Chain\n",
      "\n",
      "# Инициализация цепочки\n",
      "chain = Chain()\n",
      "\n",
      "# Добавление одного документа\n",
      "chain.add_documents([doc])\n",
      "\n",
      "# Добавление нескольких документов\n",
      "chain.add_documents(documents)\n",
      "```\n",
      "\n",
      "Таким образом, объект `Document` является основным строительным блоком для работы с неструктурированным текстом в LangChain, позволяя хранить как сам текст, так и связанную с ним информацию, что делает его важным элементом для множества задач NLP.\n",
      "\n",
      "ИНФО О ПОИСКЕ:\n",
      "Тип поиска: doc-first\n",
      "Найдено документов: 2\n",
      "Код: 0, Документация: 2\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a Document in LangChain and how to create it?\"\n",
    "\n",
    "try:\n",
    "    # Получаем ответ\n",
    "    response = retrieval_chain.invoke({\"input\": query})\n",
    "    \n",
    "    # Выводим результат\n",
    "    print(\"ОТВЕТ:\")\n",
    "    print(response[\"answer\"])\n",
    "    \n",
    "    # Показываем информацию о поиске\n",
    "    search_result = system.smart_search(query, k=3)\n",
    "    print(f\"\\nИНФО О ПОИСКЕ:\")\n",
    "    print(f\"Тип поиска: {search_result.search_type}\")\n",
    "    print(f\"Найдено документов: {len(search_result.documents)}\")\n",
    "    \n",
    "    code_count = len([d for d in search_result.documents if d.metadata.get('type') == 'code'])\n",
    "    doc_count = len([d for d in search_result.documents if d.metadata.get('type') == 'doc'])\n",
    "    print(f\"Код: {code_count}, Документация: {doc_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Ошибка: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
