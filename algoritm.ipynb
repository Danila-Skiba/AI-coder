{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f339d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# import spacy\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader, PythonLoader, DirectoryLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import ast\n",
    "\n",
    "from vectorisation import FAISSVectorStore\n",
    "\n",
    "# Инициализация spaCy для NLP-анализа\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 1. Автоматизация связывания: Поиск упоминаний .py файлов в .md файлах\n",
    "# def extract_py_references(md_content):\n",
    "#     \"\"\"Извлекает имена .py файлов из Markdown контента с помощью регулярных выражений и spaCy.\"\"\"\n",
    "#     py_files = set()\n",
    "#     # Регулярное выражение для поиска имен файлов (например, document_loader.py)\n",
    "#     pattern = r'[\\w_]+\\.py'\n",
    "#     matches = re.findall(pattern, md_content)\n",
    "#     py_files.update(matches)\n",
    "    \n",
    "#     # Использование spaCy для дополнительного анализа (поиск упоминаний файлов в контексте)\n",
    "#     doc = nlp(md_content)\n",
    "#     for ent in doc.ents:\n",
    "#         if ent.text.endswith('.py'):\n",
    "#             py_files.add(ent.text)\n",
    "    \n",
    "#     return list(py_files)\n",
    "\n",
    "# def build_file_relations(md_files, directory):\n",
    "#     \"\"\"Создает словарь связей между .md и .py файлами.\"\"\"\n",
    "#     file_relations = {}\n",
    "#     for md_file in md_files:\n",
    "#         with open(md_file, 'r', encoding='utf-8') as f:\n",
    "#             content = f.read()\n",
    "#         py_references = extract_py_references(content)\n",
    "#         # Проверяем, существуют ли упомянутые .py файлы в директории\n",
    "#         existing_py_files = [\n",
    "#             os.path.join(root, f) for root, _, files in os.walk(directory) for f in files\n",
    "#             if f in py_references and f.endswith('.py')\n",
    "#         ]\n",
    "#         file_relations[os.path.basename(md_file)] = existing_py_files\n",
    "#     return file_relations\n",
    "\n",
    "# 2. Загрузка и чанкирование .md файлов\n",
    "def load_md_files(directory):\n",
    "    \"\"\"Загружает и чанкирует .md файлы с учетом структуры заголовков.\"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        directory,\n",
    "        glob=\"**/*.md\",\n",
    "        loader_cls=UnstructuredMarkdownLoader,\n",
    "        loader_kwargs={\"mode\": \"elements\", \"strategy\": \"fast\"},\n",
    "        silent_errors=True  # Обработка ошибок: пропуск проблемных файлов\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\")]\n",
    "    splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # Оптимизация: размер чанка для больших файлов\n",
    "        chunk_overlap=200  # Оптимизация: перекрытие для сохранения контекста\n",
    "    )\n",
    "    \n",
    "    md_chunks = []\n",
    "    for doc in docs:\n",
    "        if doc.metadata.get(\"category\") == \"NarrativeText\":\n",
    "            # Чанкирование по заголовкам\n",
    "            chunks = splitter.split_text(doc.page_content)\n",
    "            for chunk in chunks:\n",
    "                chunk.metadata.update({\"source_md\": doc.metadata[\"source\"]})\n",
    "                md_chunks.append(chunk)\n",
    "        else:\n",
    "            # Для других элементов (например, код) используем текстовый сплиттер\n",
    "            chunks = text_splitter.split_text(doc.page_content)\n",
    "            for chunk in chunks:\n",
    "                md_chunks.append(Document(page_content=chunk, metadata={\"source_md\": doc.metadata[\"source\"]}))\n",
    "    \n",
    "    return md_chunks\n",
    "\n",
    "# 3. Загрузка и чанкирование .py файлов\n",
    "def load_py_files(directory):\n",
    "    \"\"\"Загружает и чанкирует .py файлы с учетом структуры кода.\"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        directory,\n",
    "        glob=\"**/*.py\",\n",
    "        loader_cls=PythonLoader,\n",
    "        silent_errors=True  # Обработка ошибок: пропуск проблемных файлов\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # Оптимизация: размер чанка для больших файлов\n",
    "        chunk_overlap=200  # Оптимизация: перекрытие для сохранения контекста\n",
    "    )\n",
    "    py_chunks = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        code = doc.page_content\n",
    "        try:\n",
    "            # Парсинг структуры Python с помощью ast\n",
    "            tree = ast.parse(code)\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                    start_line = node.lineno\n",
    "                    end_line = node.end_lineno\n",
    "                    chunk = \"\\n\".join(code.splitlines()[start_line-1:end_line])\n",
    "                    py_chunks.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\"source_py\": doc.metadata[\"source\"]}\n",
    "                    ))\n",
    "        except SyntaxError:\n",
    "            # Если синтаксис некорректен, чанкировать как текст\n",
    "            chunks = text_splitter.split_text(code)\n",
    "            for chunk in chunks:\n",
    "                py_chunks.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"source_py\": doc.metadata[\"source\"]}\n",
    "                ))\n",
    "    \n",
    "    return py_chunks\n",
    "\n",
    "# 4. Связывание чанков\n",
    "def link_chunks(md_chunks, py_chunks, directory):\n",
    "    \"\"\"Связывает чанки .md и .py файлов через автоматически созданный словарь связей.\"\"\"\n",
    "    md_files = [chunk.metadata[\"source_md\"] for chunk in md_chunks if \"source_md\" in chunk.metadata]\n",
    "    file_relations = build_file_relations(md_files, directory)\n",
    "    \n",
    "    for md_chunk in md_chunks:\n",
    "        md_file = os.path.basename(md_chunk.metadata.get(\"source_md\", \"\"))\n",
    "        if md_file in file_relations:\n",
    "            md_chunk.metadata[\"related_files\"] = file_relations[md_file]\n",
    "    \n",
    "    return md_chunks + py_chunks\n",
    "\n",
    "# 5. Сохранение в векторную базу\n",
    "def store_chunks(chunks):\n",
    "    \"\"\"Сохраняет чанки в векторной базе Chroma.\"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = Chroma.from_documents(chunks, embeddings)\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "# 6. Тестирование ретривера\n",
    "def test_retriever(retriever):\n",
    "    \"\"\"Тестирует ретривер на различных запросах.\"\"\"\n",
    "    test_queries = [\n",
    "        \"How to use UnstructuredMarkdownLoader?\",\n",
    "        \"Show example of document loading in LangChain\",\n",
    "        \"What is the purpose of RecursiveCharacterTextSplitter?\",\n",
    "        \"Code example for Chroma vector store\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nTesting query: {query}\")\n",
    "        results = retriever.get_relevant_documents(query)\n",
    "        for i, doc in enumerate(results[:2], 1):  # Ограничимся 2 результатами для краткости\n",
    "            print(f\"Result {i}:\")\n",
    "            print(f\"Content: {doc.page_content[:200]}...\")  # Первые 200 символов\n",
    "            print(f\"Metadata: {doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aea023d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"MGNlMTdlYzMtYjk3OS00ZmVlLTkzYzQtMGVmZmIyM2NkMDIzOjJlMWExNjM1LTIyZDAtNDdkMy04MmFhLTFkMDc2MDk5Y2ViMw==\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a34e8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader, PythonLoader, DirectoryLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_gigachat import GigaChatEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import ast\n",
    "from sentence_transformers import util\n",
    "import numpy as np\n",
    "\n",
    "# 1. Автоматизация связывания: Поиск упоминаний .py файлов в .md файлах\n",
    "def extract_py_references(md_content):\n",
    "    \"\"\"Извлекает имена .py файлов из Markdown контента с помощью регулярных выражений.\"\"\"\n",
    "    py_files = set()\n",
    "    pattern = r'(?:example|see|file|code)\\s+([\\w_]+\\.py)'\n",
    "    matches = re.findall(pattern, md_content, re.IGNORECASE)\n",
    "    py_files.update(matches)\n",
    "    pattern_fallback = r'[\\w_]+\\.py'\n",
    "    matches_fallback = re.findall(pattern_fallback, md_content)\n",
    "    py_files.update(matches_fallback)\n",
    "    return list(py_files)\n",
    "\n",
    "def build_file_relations(md_files, directory):\n",
    "    \"\"\"Создает словарь связей между .md и .py файлами.\"\"\"\n",
    "    file_relations = {}\n",
    "    for md_file in md_files:\n",
    "        with open(md_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        py_references = extract_py_references(content)\n",
    "        existing_py_files = [\n",
    "            os.path.join(root, f) for root, _, files in os.walk(directory) for f in files\n",
    "            if f in py_references and f.endswith('.py')\n",
    "        ]\n",
    "        file_relations[os.path.basename(md_file)] = existing_py_files\n",
    "    return file_relations\n",
    "\n",
    "# # 2. Семантическое связывание с GigaChatEmbeddings\n",
    "# def link_chunks_semantically(md_chunks, py_chunks, credentials, scope=\"GIGACHAT_API_PERS\"):\n",
    "#     \"\"\"Связывает чанки .md и .py на основе семантического сходства.\"\"\"\n",
    "#     embeddings = GigaChatEmbeddings(credentials=credentials, scope=scope, verify_ssl_certs=False)\n",
    "    \n",
    "#     # Фильтрация и разбиение длинных чанков\n",
    "#     max_tokens = 500  # Чуть меньше лимита GigaChat\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "    \n",
    "#     filtered_md_chunks = []\n",
    "#     for chunk in md_chunks:\n",
    "#         if len(chunk.page_content) > max_tokens:\n",
    "#             split_chunks = text_splitter.split_text(chunk.page_content)\n",
    "#             for split_chunk in split_chunks:\n",
    "#                 filtered_md_chunks.append(Document(\n",
    "#                     page_content=split_chunk,\n",
    "#                     metadata=chunk.metadata\n",
    "#                 ))\n",
    "#         else:\n",
    "#             filtered_md_chunks.append(chunk)\n",
    "    \n",
    "#     filtered_py_chunks = []\n",
    "#     for chunk in py_chunks:\n",
    "#         if len(chunk.page_content) > max_tokens:\n",
    "#             split_chunks = text_splitter.split_text(chunk.page_content)\n",
    "#             for split_chunk in split_chunks:\n",
    "#                 filtered_py_chunks.append(Document(\n",
    "#                     page_content=split_chunk,\n",
    "#                     metadata=chunk.metadata\n",
    "#                 ))\n",
    "#         else:\n",
    "#             filtered_py_chunks.append(chunk)\n",
    "    \n",
    "#     # Получение эмбеддингов\n",
    "#     md_texts = [chunk.page_content for chunk in filtered_md_chunks]\n",
    "#     py_texts = [chunk.page_content for chunk in filtered_py_chunks]\n",
    "#     md_embeddings = embeddings.embed_documents(md_texts)\n",
    "#     py_embeddings = embeddings.embed_documents(py_texts)\n",
    "    \n",
    "#     similarities = util.cos_sim(md_embeddings, py_embeddings).numpy()\n",
    "    \n",
    "#     threshold = 0.75\n",
    "#     for i, md_chunk in enumerate(filtered_md_chunks):\n",
    "#         related_files = md_chunk.metadata.get(\"related_files\", [])\n",
    "#         for j, py_chunk in enumerate(filtered_py_chunks):\n",
    "#             if similarities[i][j] > threshold:\n",
    "#                 related_files.append(py_chunk.metadata[\"source_py\"])\n",
    "#         md_chunk.metadata[\"related_files\"] = list(set(related_files))\n",
    "    \n",
    "#     return filtered_md_chunks + filtered_py_chunks\n",
    "\n",
    "def link_chunks_semantically(md_chunks, py_chunks):\n",
    "    \"\"\"Связывает чанки .md и .py на основе семантического сходства.\"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    md_texts = [chunk.page_content for chunk in md_chunks]\n",
    "    py_texts = [chunk.page_content for chunk in py_chunks]\n",
    "    md_embeddings = embeddings.embed_documents(md_texts)\n",
    "    py_embeddings = embeddings.embed_documents(py_texts)\n",
    "    \n",
    "    similarities = util.cos_sim(md_embeddings, py_embeddings).numpy()\n",
    "    \n",
    "    threshold = 0.75\n",
    "    for i, md_chunk in enumerate(md_chunks):\n",
    "        related_files = md_chunk.metadata.get(\"related_files\", [])\n",
    "        for j, py_chunk in enumerate(py_chunks):\n",
    "            if similarities[i][j] > threshold:\n",
    "                related_files.append(py_chunk.metadata[\"source_py\"])\n",
    "        md_chunk.metadata[\"related_files\"] = list(set(related_files))\n",
    "    \n",
    "    return md_chunks + py_chunks\n",
    "\n",
    "# 3. Загрузка и чанкирование .md файлов\n",
    "def load_md_files(directory):\n",
    "    \"\"\"Загружает и чанкирует .md файлы с учетом структуры заголовков.\"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        directory,\n",
    "        glob=\"**/*.md\",\n",
    "        loader_cls=UnstructuredMarkdownLoader,\n",
    "        loader_kwargs={\"mode\": \"elements\", \"strategy\": \"fast\"},\n",
    "        silent_errors=True\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\")]\n",
    "    splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)  # Уменьшен chunk_size\n",
    "    \n",
    "    md_chunks = []\n",
    "    for doc in docs:\n",
    "        if doc.metadata.get(\"category\") == \"NarrativeText\":\n",
    "            chunks = splitter.split_text(doc.page_content)\n",
    "            for chunk in chunks:\n",
    "                chunk.metadata.update({\"source_md\": doc.metadata[\"source\"], \"content_type\": \"docs\"})\n",
    "                md_chunks.append(chunk)\n",
    "        else:\n",
    "            chunks = text_splitter.split_text(doc.page_content)\n",
    "            for chunk in chunks:\n",
    "                md_chunks.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"source_md\": doc.metadata[\"source\"], \"content_type\": \"docs\"}\n",
    "                ))\n",
    "    \n",
    "    return md_chunks\n",
    "\n",
    "# 4. Загрузка и чанкирование .py файлов\n",
    "def load_py_files(directory):\n",
    "    \"\"\"Загружает и чанкирует .py файлы с учетом структуры кода.\"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        directory,\n",
    "        glob=\"**/*.py\",\n",
    "        loader_cls=PythonLoader,\n",
    "        silent_errors=True\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    py_chunks = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        code = doc.page_content\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                    start_line = node.lineno\n",
    "                    end_line = node.end_lineno\n",
    "                    chunk = \"\\n\".join(code.splitlines()[start_line-1:end_line])\n",
    "                    py_chunks.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\"source_py\": doc.metadata[\"source\"], \"content_type\": \"code\"}\n",
    "                    ))\n",
    "        except SyntaxError:\n",
    "            chunks = text_splitter.split_text(code)\n",
    "            for chunk in chunks:\n",
    "                py_chunks.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"source_py\": doc.metadata[\"source\"], \"content_type\": \"code\"}\n",
    "                ))\n",
    "    \n",
    "    return py_chunks\n",
    "\n",
    "# 5. Связывание чанков\n",
    "def link_chunks(md_chunks, py_chunks, directory, credentials = None, scope=\"GIGACHAT_API_PERS\"):\n",
    "    \"\"\"Связывает чанки .md и .py файлов через регулярные выражения и семантическое сходство.\"\"\"\n",
    "    md_files = [chunk.metadata[\"source_md\"] for chunk in md_chunks if \"source_md\" in chunk.metadata]\n",
    "    file_relations = build_file_relations(md_files, directory)\n",
    "    \n",
    "    for md_chunk in md_chunks:\n",
    "        md_file = os.path.basename(md_chunk.metadata.get(\"source_md\", \"\"))\n",
    "        if md_file in file_relations:\n",
    "            md_chunk.metadata[\"related_files\"] = file_relations[md_file]\n",
    "    \n",
    "    all_chunks = link_chunks_semantically(md_chunks, py_chunks)\n",
    "    return all_chunks\n",
    "\n",
    "# 6. Сохранение в векторную базу\n",
    "def store_chunks(chunks: List[Document]):\n",
    "    print(f\"Processing {len(chunks)} chunks\")\n",
    "    if not chunks:\n",
    "        raise ValueError(\"Список чанков пуст, невозможно создать векторную базу\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "    print(\"Создана новая векторная база FAISS\")\n",
    "    return vector_store.as_retriever()\n",
    "\n",
    "# 7. Тестирование ретривера с учетом предпочтений пользователя\n",
    "def test_retriever(retriever, queries=None):\n",
    "    \"\"\"Тестирует ретривер на различных запросах с учетом типа контента.\"\"\"\n",
    "    if queries is None:\n",
    "        queries = [\n",
    "            \"Show a code example of creating a ReAct agent using LangGraph's create_react_agent function.\",  # Предпочтение: документация\n",
    "            \"Provide a code example of using the ainvoke method to asynchronously invoke a LangChain Runnable.\",  # Предпочтение: код\n",
    "            \"Show a code example of using SelfQueryRetriever to convert a natural language query into metadata filters.\",  # Предпочтение: документация\n",
    "            \"What is the purpose of the langchain-core package in the LangChain framework?\" \n",
    "            \"What is the difference between older string-in, string-out LLMs and newer Chat Models in LangChain?\"\n",
    "        ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nTesting query: {query}\")\n",
    "        # Определяем предпочтение: код или документация\n",
    "        prefer_code = any(keyword in query.lower() for keyword in [\"code\", \"example\", \"script\"])\n",
    "        content_type = \"code\" if prefer_code else \"docs\"\n",
    "        \n",
    "        results = retriever.get_relevant_documents(query)\n",
    "        # Фильтрация результатов по типу контента\n",
    "        filtered_results = [\n",
    "            doc for doc in results if doc.metadata.get(\"content_type\") == content_type\n",
    "        ] or results[:2]  # Возвращаем все, если фильтр не дал результатов\n",
    "        \n",
    "        for i, doc in enumerate(filtered_results[:2], 1):\n",
    "            print(f\"Result {i}:\")\n",
    "            print(f\"Content: {doc.page_content}\")\n",
    "            print(f\"Metadata: {doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c992368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(directory, credentials, scope=\"GIGACHAT_API_PERS\"):\n",
    "    \"\"\"Основной процесс обработки документации.\"\"\"\n",
    "    md_chunks = load_md_files(f\"{directory}/md\")\n",
    "    py_chunks = load_py_files(f\"{directory}/py\")\n",
    "    all_chunks = link_chunks(md_chunks, py_chunks, directory, credentials, scope)\n",
    "    retriever = store_chunks(all_chunks)\n",
    "    test_retriever(retriever)\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40f2fd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 298 chunks\n",
      "Создана новая векторная база FAISS\n",
      "\n",
      "Testing query: Show a code example of creating a ReAct agent using LangGraph's create_react_agent function.\n",
      "Result 1:\n",
      "Content: def create_react_agent(\n",
      "    llm: BaseLanguageModel,\n",
      "    tools: Sequence[BaseTool],\n",
      "    prompt: BasePromptTemplate,\n",
      "    output_parser: Optional[AgentOutputParser] = None,\n",
      "    tools_renderer: ToolsRenderer = render_text_description,\n",
      "    *,\n",
      "    stop_sequence: Union[bool, list[str]] = True,\n",
      ") -> Runnable:\n",
      "    \"\"\"Create an agent that uses ReAct prompting.\n",
      "\n",
      "    Based on paper \"ReAct: Synergizing Reasoning and Acting in Language Models\"\n",
      "    (https://arxiv.org/abs/2210.03629)\n",
      "\n",
      "    .. warning::\n",
      "       This implementation is based on the foundational ReAct paper but is older and not well-suited for production applications.\n",
      "       For a more robust and feature-rich implementation, we recommend using the `create_react_agent` function from the LangGraph library.\n",
      "       See the [reference doc](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)\n",
      "       for more information.\n",
      "\n",
      "    Args:\n",
      "        llm: LLM to use as the agent.\n",
      "        tools: Tools this agent has access to.\n",
      "        prompt: The prompt to use. See Prompt section below for more.\n",
      "        output_parser: AgentOutputParser for parse the LLM output.\n",
      "        tools_renderer: This controls how the tools are converted into a string and\n",
      "            then passed into the LLM. Default is `render_text_description`.\n",
      "        stop_sequence: bool or list of str.\n",
      "            If True, adds a stop token of \"Observation:\" to avoid hallucinates.\n",
      "            If False, does not add a stop token.\n",
      "            If a list of str, uses the provided list as the stop tokens.\n",
      "\n",
      "            Default is True. You may to set this to False if the LLM you are using\n",
      "            does not support stop sequences.\n",
      "\n",
      "    Returns:\n",
      "        A Runnable sequence representing an agent. It takes as input all the same input\n",
      "        variables as the prompt passed in does. It returns as output either an\n",
      "        AgentAction or AgentFinish.\n",
      "\n",
      "    Examples:\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain import hub\n",
      "            from langchain_community.llms import OpenAI\n",
      "            from langchain.agents import AgentExecutor, create_react_agent\n",
      "\n",
      "            prompt = hub.pull(\"hwchase17/react\")\n",
      "            model = OpenAI()\n",
      "            tools = ...\n",
      "\n",
      "            agent = create_react_agent(model, tools, prompt)\n",
      "            agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
      "\n",
      "            agent_executor.invoke({\"input\": \"hi\"})\n",
      "\n",
      "            # Use with chat history\n",
      "            from langchain_core.messages import AIMessage, HumanMessage\n",
      "            agent_executor.invoke(\n",
      "                {\n",
      "                    \"input\": \"what's my name?\",\n",
      "                    # Notice that chat_history is a string\n",
      "                    # since this prompt is aimed at LLMs, not chat models\n",
      "                    \"chat_history\": \"Human: My name is Bob\\\\nAI: Hello Bob!\",\n",
      "                }\n",
      "            )\n",
      "\n",
      "    Prompt:\n",
      "\n",
      "        The prompt must have input keys:\n",
      "            * `tools`: contains descriptions and arguments for each tool.\n",
      "            * `tool_names`: contains all tool names.\n",
      "            * `agent_scratchpad`: contains previous agent actions and tool outputs as a string.\n",
      "\n",
      "        Here's an example:\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain_core.prompts import PromptTemplate\n",
      "\n",
      "            template = '''Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "            {tools}\n",
      "\n",
      "            Use the following format:\n",
      "\n",
      "            Question: the input question you must answer\n",
      "            Thought: you should always think about what to do\n",
      "            Action: the action to take, should be one of [{tool_names}]\n",
      "            Action Input: the input to the action\n",
      "            Observation: the result of the action\n",
      "            ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "            Thought: I now know the final answer\n",
      "            Final Answer: the final answer to the original input question\n",
      "\n",
      "            Begin!\n",
      "\n",
      "            Question: {input}\n",
      "            Thought:{agent_scratchpad}'''\n",
      "\n",
      "            prompt = PromptTemplate.from_template(template)\n",
      "    \"\"\"  # noqa: E501\n",
      "    missing_vars = {\"tools\", \"tool_names\", \"agent_scratchpad\"}.difference(\n",
      "        prompt.input_variables + list(prompt.partial_variables)\n",
      "    )\n",
      "    if missing_vars:\n",
      "        raise ValueError(f\"Prompt missing required variables: {missing_vars}\")\n",
      "\n",
      "    prompt = prompt.partial(\n",
      "        tools=tools_renderer(list(tools)),\n",
      "        tool_names=\", \".join([t.name for t in tools]),\n",
      "    )\n",
      "    if stop_sequence:\n",
      "        stop = [\"\\nObservation\"] if stop_sequence is True else stop_sequence\n",
      "        llm_with_stop = llm.bind(stop=stop)\n",
      "    else:\n",
      "        llm_with_stop = llm\n",
      "    output_parser = output_parser or ReActSingleInputOutputParser()\n",
      "    agent = (\n",
      "        RunnablePassthrough.assign(\n",
      "            agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
      "        )\n",
      "        | prompt\n",
      "        | llm_with_stop\n",
      "        | output_parser\n",
      "    )\n",
      "    return agent\n",
      "Metadata: {'source_py': 'test_directory\\\\py\\\\agent_1.py', 'content_type': 'code'}\n",
      "\n",
      "\n",
      "Testing query: Provide a code example of using the ainvoke method to asynchronously invoke a LangChain Runnable.\n",
      "Result 1:\n",
      "Content: Runnables created using the LangChain Expression Language (LCEL) can also be run asynchronously as they implement the full Runnable Interface.\n",
      "Metadata: {'source_md': 'test_directory\\\\md\\\\docs_concepts_async.md', 'content_type': 'docs', 'related_files': []}\n",
      "\n",
      "Result 2:\n",
      "Content: Many components of LangChain implement the Runnable Interface, which includes support for asynchronous execution. This means that you can run Runnables asynchronously using the await keyword in Python.\n",
      "Metadata: {'source_md': 'test_directory\\\\md\\\\docs_concepts_async.md', 'content_type': 'docs', 'related_files': []}\n",
      "\n",
      "\n",
      "Testing query: Show a code example of using SelfQueryRetriever to convert a natural language query into metadata filters.\n",
      "Result 1:\n",
      "Content: As an example, here is how to use the SelfQueryRetriever to convert natural language queries into metadata filters.\n",
      "Metadata: {'source_md': 'test_directory\\\\md\\\\docs_concepts_retrieval.md', 'content_type': 'docs', 'related_files': []}\n",
      "\n",
      "Result 2:\n",
      "Content: Natural Language to Metadata Filters: Converts user queries into appropriate metadata filters.\n",
      "Metadata: {'source_md': 'test_directory\\\\md\\\\docs_concepts_retrieval.md', 'content_type': 'docs', 'related_files': []}\n",
      "\n",
      "\n",
      "Testing query: What is the purpose of the langchain-core package in the LangChain framework?What is the difference between older string-in, string-out LLMs and newer Chat Models in LangChain?\n",
      "Result 1:\n",
      "Content: langchain-core​\n",
      "Metadata: {'source_md': 'test_directory\\\\md\\\\docs_concepts_architecture.md', 'content_type': 'docs', 'related_files': []}\n",
      "\n",
      "Result 2:\n",
      "Content: LangChain has implementations for older language models that take a string as input and return a string as output. These models are typically named without the \"Chat\" prefix (e.g., Ollama, Anthropic, OpenAI, etc.), and may include the \"LLM\" suffix (e.g., OllamaLLM, AnthropicLLM, OpenAILLM, etc.). These models implement the BaseLLM interface.\n",
      "Metadata: {'source_md': 'test_directory\\\\md\\\\docs_concepts_text_llms.md', 'content_type': 'docs', 'related_files': []}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directory = \"test_directory\" \n",
    "credentials = API_KEY\n",
    "scope = \"GIGACHAT_API_PERS\"\n",
    "retriever = main(directory, credentials, scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fdcdcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
